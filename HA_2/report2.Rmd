---
title: "Home Assignment 2"
author: "Alfons"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Part 1 

In part 1 we will implement and practice Principal Components Analysis (PCA) in R. We will use the handwritten digit data set to solve the following questions.

To read the data the into a data matrix in R the following code was run:

```{r}
data = read.table(gzfile("zip.train"))
data <- as.matrix(data)
```


## Task 1

Read the data set into R. Choose your favorite (or lucky) number and use the subset of images of this number as a working data matrix for the tasks 1 to 4. Use function ’image’ to display the first 24 cases of your lucky number.

**Answer**:

We chose the number 4 to use in the following tasks. To get only the rows containing information about 4's:

```{r}
Four <- data[which(data[,1]==4),2:257]

```

Finally, we plot the 24 first images:

```{r echo=FALSE}
colors <- c('white','black'); 
cus_col <- colorRampPalette(colors=colors)

par(mfrow= c(4,6))
par(mar=c(1, 1, 1, 1))

for (i in 1:24) {
  z <- matrix(Four[i,256:1],16,16,byrow=T)[,16:1]
  image(t(z), col=cus_col(256))
}
```

## Task 2
Do PCA on your working data matrix. Display the first 4 and the last 4 principal vectors (eigenvectors) as 16 × 16 images.

**Answer**:

To get the eigenvectors, we first get the covariance matrix S, and then do a eigen decomposition:

```{r}
S = cov(Four)
eigen_dec = eigen(S)
```

Then, the first 4 principal components are plotted by taking the first columns (1 x 256) and put it into a matrix (16 x 16). These first 4 matrices are then plotted:

```{r echo=FALSE}
par(mfrow=c(2, 2))
par(mar=c(1, 1, 1, 1))

for (i in 1:4){
  p_vec = eigen_dec$vectors[, i]
  p_matrix = matrix(p_vec[256:1], 16, 16, byrow=T)[,16:1]
  image(t(p_matrix), col=cus_col(256))
}
```

Similarly, the last 4 principal component can be plotted:

```{r echo=FALSE}
par(mfrow=c(2, 2))
par(mar=c(1,1,1,1))
for (i in 253:256) {
  p_vec = eigen_dec$vectors[, i]
  p_matrix = matrix(p_vec[256:1], 16, 16, byrow=T)[,16:1]
  image(t(p_matrix), col=cus_col(256))
}
```

We can see a large difference in the amount of information the first 4 principal components contains about the number 4 when compared to the last 4 principal components, which contains very little at all, and almost none about the number 4. 

## Task 3

Image reconstruction. Approximate one image in your data set by its first 30, 60, 100, 150, and 200 principal components separately. Put the original image and five approximated images in one plot. For each approximated image, calculate and report the mean square errors, $\frac{||x − \hat{x}||^2}{p}$, where $x_ {p×1}$ denotes the original image, $\hat{x}_{p×1}$ denotes the approximated image, and p = 256 is the number of pixels in a image.

**Answer**:

To approximate an image using a given amount of principal components we created this function:

```{r}
get_approx = function(obs, n, e_mat) {
  approximation = 0 
  for (i in 1:n) {
    yi = t(obs) %*% e_mat$vectors[, i]
    pcomp = yi %*% e_mat$vectors[, i]
    approximation = approximation + pcomp
  }
  return(approximation)
}

```

This function approximates the image by projecting the observation to n principal components and then adding the approximations together. The principal component $y_i$ was calculated by  $y_j = x'\mathbf{q}_j$ where $\mathbf{q}$ is the j-th principal component (1x256 vector), x is a 256x1 vector. The approximation $x_i$ is then obtained by $x_i= \sum_{j=1}^n y_j\mathbf{q}_j$. 

We chose to approximate the first 4 in our dataset. Then the original image was plotted followed by approximations using 30, 60, 100, 150, 200 principal components in order. For each approximation the mean square error (MSE) was calculated by $\frac{||x − \hat{x}||^2}{p}$.

```{r echo=FALSE}
par(mfrow=c(2,3))
par(mar=c(1,1,1,1))
z <- matrix(Four[1,256:1],16,16,byrow=T)[,16:1]
image(t(z), col=cus_col(256))

for (i in c(30, 60, 100, 150, 200)) {
  approximation = get_approx(Four[1, ], i , eigen_dec)
  approx_mat = matrix(approximation[256:1], 16, 16, byrow=T)[,16:1]
  print((norm(t(t(Four[1, ])) - t(approximation))^2)/256)
  image(t(approx_mat), col=cus_col(256))
}
```

It can be seen that the MSE drops a lot between the observations with 30, 60 and 100 observations, but after that the starts to decrease a bit slower. This also seems to be the case in the plots of the approximations, where the first 3 plots are a bit "blurry" but the last 2 look very close to the original image. 

## Task 4
The total variation, or all of the information, is given by the sum of all eigenvalues. The function cutoff85 below uses the formula:
$\frac{\sum_{i=1}^{q}\lambda_i}{\sum_{i=1}^{p}\lambda_i} = 85\%$
to calculate the number of principal components required for keeping 85% of the total information (or variation).
```{r}
cutoff85 <- function(){
  tot_eigv = sum(eigen_dec[["values"]])
  i = 1
  cond = 0
  while(cond <= 0.85){
    eigv = sum(eigen_dec[["values"]][1:i])
    cond = eigv/tot_eigv
    i = i + 1
  }
  return(i-1)
}
```
The cutoff85 function stops once the cond is equal or bigger than 0.85 and returns the number of principal components used.
```{r}
num_pcs <- cutoff85()
num_pcs # number of pcs required for keeping  85% the information
```
To keep 85% of the total information we only need 33 principal components.


## Task 5
In this task we will build a Gaussian mixture model using the first two principal components. This GMM will then be used to classify images as a five or six. First, all the images of fives and sixes need to be separated from the rest, then the all the images of fives and sixes need to be separated into a training set and testing set. The eigendecomposition is done the on covariance matrix from the training set, the eigenvectors from the decompistion will then be used to calculate the first two principal components for both the training and testing set.

```{r}
# Task 5
Five_Six <- data[which(data[,1]==5 | data[,1] == 6),2:257]
Five_Six_w_labels <- data[which(data[,1]==5 | data[,1] == 6),1:257]
n = dim(Five_Six)[1] # sample size
set.seed(2022)
id = sample(1:n, floor(0.8*n)) 
train_data = Five_Six[id, ] # training set
test_data = Five_Six[-id, ] # testing set
S = cov(train_data) # training covariance matrix
mu = colMeans(train_data) # mean vector
# eigen decomposition of covariance matrix
eigen_dec = eigen(S) 
eigen_val = eigen_dec$values
eigen_vec = eigen_dec$vectors
```

The first two principal components from the decomposition:
```{r}
# Get the first 2 principal components for train data
PC1 = numeric(length(train_data[, 1]))
PC2 = numeric(length(train_data[, 1]))
```

Calculate the first two principal components for all images in the training set:
```{r}
for (i in 1:length(train_data[, 1])){
  z1 = t(train_data[i, ] - mu) %*% eigen_vec[, 1]
  z2 = t(train_data[i, ] - mu) %*% eigen_vec[, 2]
  
  PC1[i] = z1
  PC2[i] = z2
}
```

We then create a new dataframe containing only the labels as well as the principal components:
```{r}
# create new dataframe with labels and principal components for train data
train_labels = Five_Six_w_labels[id, 1]
test_dat = data.frame(train_labels, PC1, PC2)
```

This new dataframe is separated further into two dataframes: train_fives and train_sixs:
```{r}
# separate 5s and 6s
train_fives = test_dat[which(test_dat[,1]==5),2:3]
train_sixs = test_dat[which(test_dat[,1]==6),2:3]
```

This is done in order to calculate the covariance matrix and mean vectors for both groups (5s and 6s), these will then be used in the GMM classifier for prediction.
```{r}
# Get statistics for 5s and 6s
mu5 = colMeans(train_fives)
S5 = cov(train_fives)
mu6 = colMeans(train_sixs)
S6 = cov(train_sixs)
```

The principal components for the testing data is handled the same way as the training data, using the same eigenvectors being the important part.
```{r}
# Get test data
test_mu = colMeans(test_data)
test_PC1 = numeric(length(test_data[, 1]))
test_PC2 = numeric(length(test_data[, 1]))
for (i in 1:length(test_data[, 1])) {
  z1 = t(test_data[i, ]- test_mu) %*% eigen_vec[, 1]
  z2 = t(test_data[i, ]- test_mu) %*% eigen_vec[, 2]
  
  test_PC1[i] = z1
  test_PC2[i] = z2
}
```

We then build a new dataframe (like we did for the training data), only consisting of labels and the principal components.
```{r}
# create new dataframe with labels and principal components for test data
test_labels = Five_Six_w_labels[-id, 1]
test_dat = data.frame(test_labels, test_PC1, test_PC2)
```

The new dataframes are done, classification soon be done. The following functions are repurposed from HA1, with slight modifications.
```{r}
# classifier functions
exponent = function(my_vector, row_observation, cov_matrix){
  x = as.vector(row_observation) - as.vector(my_vector)
  result = exp(-t(t(x)) %*% solve(cov_matrix) %*% t(x)/2)
  return(result)
}
# The base/quota of the multinomial distribution
base = function(cov_matrix, p){
  result <- 1/(((2*pi)^(p/2))*sqrt(det(cov_matrix)))
  return(result)
}
# classify an observation as malignant=1, benign = 0
classifier <- function(row_observation, five_prob, six_prob){
  five <- base(S5, length(row_observation)) * exponent(mu5, row_observation, S5) * five_prob
  six <- base(S6, length(row_observation)) * exponent(mu6, row_observation, S6) * six_prob
  if (six > five){
    prediction <- 6
  } else{
    prediction <- 5
  }
  return(prediction)
}
```

If the image is predicted to be a five, the numbers five is returned, if the image is predicted to be a six, a six is returned. Before we can start classifying we need the initial probabilities of getting a five and six, respectively.
```{r}
five_prob = length(train_fives[,1]) / length(train_data[, 1]) 
six_prob = length(train_sixs[,1]) / length(train_data[, 1])
```

The classifying is done in the for-loop below.
```{r}
num_five = 0
num_six = 0
pred_col <- c()
for (i in 1:nrow(test_dat)) {
  guess <- classifier(test_dat[i, 2:3], five_prob, six_prob) # return 5 or 6
  if(guess == 5){ # guess is 5=five
    num_five = num_five + 1
    pred_col <- append(pred_col, 5)
  }else{ # guess is 6=six
    num_six = num_six + 1
    pred_col <- append(pred_col, 6)
  }
}
```

Print the number of fives and sixes.
```{r}
# Print number of malignant and benign in test data
num_five
num_six
```

The accuracy of the classifier is calculated by going through the test data and see if the label (a 5 or 6) matches the number given by the classifier.
```{r}
# Calculate the classifier accuracy
cor_pred <- 0
wr_pred <- 0
for (i in 1:length(pred_col)){
  if(pred_col[i]==5 & test_dat[i,1]==5){ # if predict = 1 and diagnosis = "M"
    cor_pred <- cor_pred + 1
  }
  if(pred_col[i]==6 & test_dat[i,1]==6){ # if predict = 0 and diagnosis = "B"
    cor_pred <- cor_pred + 1
  }
}
```

The accuracy of the classifier:
```{r}
accuracy = cor_pred / length(pred_col)
accuracy
cor_pred
```

The classifier achieves a very high accuracy, despite only using two principal components. This accuracy was achieved by only using the first two principal components, and to call back to a previous task, we needed around 30 principal components to keep 85% of the information. If the classifier was not using principal components, but all of the 256 variables instead, we highly doubt the classifier would achieve this accuracy by only using two of the variables. By using principal components, we reduced the number of variables from 256 to only two, thus demonstrating the power of dimension reduction.



